# RKE快速部署kubernetes集群

官网地址：https://rancher.com/

中文地址：https://docs.rancher.cn/

## 前期准备

### 主机准备

| address         | hostname | system    | user    | role                      |
| --------------- | -------- | --------- | ------- | ------------------------- |
| 192.168.168.131 | vm1      | CentOS7.7 | rancher | [controlplan,worker,etcd] |
| 192.168.168.132 | vm2      | CentOS7.7 | rancher | [controlplan,worker,etcd] |
| 192.168.168.133 | vm3      | CentOS7.7 | rancher | [controlplan,worker,etcd] |
| 192.168.168.134 | vm4      | CentOS7.7 | rancher | [worker]                  |

### 环境准备

#### 执行系统优化脚本

```
#也可不执行，自行优化
sh  system.sh
cat system.sh
#############################################################
# File Name: system.sh
# Author: robin
# Created Time: Fri 18 Oct 2018 05:01:02 PM CST
#==================================================================
#!/bin/sh
# 运行环境CentOS 7.x

echo "判断centos7还是centos6系统"
sleep 1
VERSION=`cat /etc/redhat-release|awk -F " " '{print $3}'|awk -F "." '{print $1}'`
if [ "$VERSION" == "6" ];then
VERSION='6'
echo "centos6"
else
VERSION='7'
echo "centos7"
fi

echo "查看内存 cpu 硬盘大小"
sleep 1
MEM=`free -m`
#4.1查看物理CPU个数
physical_id=`cat /proc/cpuinfo| grep "physical id"| sort| uniq| wc -l`
#4.2查看每个物理CPU中core的个数(即核数)
cpu_cores=`cat /proc/cpuinfo| grep "cpu cores"| uniq`
#4.3查看逻辑CPU的个数
processor=`cat /proc/cpuinfo| grep "processor"| wc -l`
echo "$MEM"
echo "####################################################"
echo "cpu物理个数 physical_id:          $physical_id"
echo "每个cpu中core的个数(即核数)       $cpu_cores"
echo "逻辑cpu的个数 processor:          $processor"
echo "####################################################"
#4.4硬盘大小
disk=`df -Th`
echo "$disk"

echo "下载wget，请稍后·······"
yum -y install wget   &>/dev/null

echo "添加DNS地址，请稍等....... "
cat >> /etc/resolv.conf << EOF
nameserver 114.114.114.114 
nameserver 114.114.114.114 
EOF

echo "是否换成阿里云的的源 "
echo "等待3秒:"
sleep 3
cat << EOF
        **********************
        1.[change aliyuan]
        2.[no change aliyuan]
        3.[exit]
    pls input the num you want:
        **********************
EOF
read -t 30 -p "pls input the num you want:" a
[ -n "`echo $a|sed 's#[0-9]##g'`" ] && {
         echo "Input error"
        exit 1
}
iffuncation(){
if [ $a -eq 1 ];then
        echo "change aliyuan"
        echo "等待3S"
        sleep 3
        mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup   &>/dev/null
        if [ "$VERSION" == "7" ];then
        wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo   &>/dev/null
        else
        wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo   &>/dev/null
        fi
        yum clean all    &>/dev/null
        yum makecache    &>/dev/null
        echo "等待3S" 
        sleep 3
elif [ $a -eq 2 ];then
        echo "no change aliyuan"
elif [ $a -eq 3 ];then
        exit 1
else
        echo "Input error"
        exit 1
fi
}
iffuncation

echo "下载必要的初始化的工具"
        sleep 3
        yum -y install net-tools tree nmap lrzsz dos2unix telnet screen vim lsof wget ntp rsync   &>/dev/null

echo "修改ip和主机名的对应关系 /etc/hosts"
        sleep 3
cat > /etc/hosts << EOF
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
EOF
if [ "$VERSION" == "7" ];then
        echo "`ifconfig|sed -n '2p'|awk -F " " '{print $2}'` $HOSTNAME" >> /etc/hosts
else
        echo "`ifconfig|sed -n '2p'|awk -F " " '{print $2}'|awk -F ":" '{print $2}'` $HOSTNAME" >> /etc/hosts
fi

echo "查看时间 并设置初始化时间"
date +%F\ %T
ntpdate cn.pool.ntp.org && hwclock -w

echo "命令补全,请稍等....... "
yum install bash-completion -y &>/dev/null

echo "/设置linux的最大文件打开数"
ulimit -SHn 65535
ulimit -a
if [ "`egrep "* - nofile 65535|* - nproc 65536" /etc/security/limits.conf|wc -l`" == "0" ];then
        echo "* - nofile 65535" >> /etc/security/limits.conf
        echo "* - nproc 65536" >> /etc/security/limits.conf
else
        echo "linux的最大文件打开数 设置成功或者之前已经设置过了"
fi
sleep 2

echo "禁用selinux,请稍等......."
sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config
setenforce 0

echo "关闭防火墙,请稍等......."
systemctl disable firewalld.service   &>/dev/null
systemctl stop firewalld.service

echo "优化ssh服务,请稍等......."
sed -i 's/^GSSAPIAuthentication yes$/GSSAPIAuthentication no/' /etc/ssh/sshd_config
sed -i 's/#UseDNS yes/UseDNS no/' /etc/ssh/sshd_config
systemctl restart sshd.service

echo "-------<内核参数优化>-------"
cat >> /etc/sysctl.conf << EOF
net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv4.icmp_echo_ignore_broadcasts = 1
net.ipv4.icmp_ignore_bogus_error_responses = 1
net.ipv4.ip_forward = 0
net.ipv4.conf.all.send_redirects = 0
net.ipv4.conf.default.send_redirects = 0
net.ipv4.conf.all.rp_filter = 1
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.accept_source_route = 0
kernel.sysrq = 0
kernel.core_uses_pid = 1
net.ipv4.tcp_syncookies = 1
kernel.msgmnb = 65536
kernel.msgmax = 65536
kernel.shmmax = 68719476736
kernel.shmall = 4294967296
net.ipv4.tcp_max_tw_buckets = 6000
net.ipv4.tcp_sack = 1
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_rmem = 4096 87380 4194304
net.ipv4.tcp_wmem = 4096 16384 4194304
net.core.wmem_default = 8388608
net.core.rmem_default = 8388608
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.core.netdev_max_backlog = 262144
net.ipv4.tcp_max_orphans = 3276800
net.ipv4.tcp_max_syn_backlog = 262144
net.ipv4.tcp_timestamps = 0
net.ipv4.tcp_synack_retries = 1
net.ipv4.tcp_syn_retries = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_mem = 94500000 915000000 927000000
net.ipv4.tcp_fin_timeout = 1
net.ipv4.tcp_keepalive_time = 30
#net.ipv4.ip_local_port_range = 1024 65000
fs.file-max = 265535
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.default.accept_redirects = 0
net.ipv4.conf.all.secure_redirects = 0
net.ipv4.conf.default.secure_redirects = 0
vm.swappiness = 10

EOF
/sbin/sysctl -p
echo "系统已优化完毕，请使用！"
```

#### 开放端口

```
#通常在Kubernetes节点上打开下列端口，测试环境可以直接关闭firewalld
ansible hosts -m shell -a 'systemctl stop firewalld && systemctl status firewalld'
```

| TCP     | 22           | Node driver SSH provisioning                        |
| ------- | ------------ | --------------------------------------------------- |
| TCP     | 2376         | Node driver Docker daemon TLS port                  |
| TCP     | 2379         | etcd client requests                                |
| TCP     | 2380         | etcd peer communication                             |
| UDP     | 8472         | Canal/Flannel VXLAN overlay networking              |
| UDP     | 4789         | Flannel VXLAN overlay networking on Windows cluster |
| TCP     | 9099         | Canal/Flannel livenessProbe/readinessProbe          |
| TCP     | 6783         | Weave Port                                          |
| UDP     | 6783-6784    | Weave UDP Ports                                     |
| TCP     | 10250        | kubelet API                                         |
| TCP     | 10254        | Ingress controller livenessProbe/readinessProbe     |
| TCP/UDP | 30000- 32767 | NodePort port range                                 |

#### 关闭selinux

```
ansible vms -m shell -a 'setenforce 0 && getenforce'
ansible vms -m shell -a 'sed -i '/^SELINUX=/s/enforcing/disabled/' /etc/selinux/config'
```

#### 禁用swap

```sh
#一定要禁用swap，否则kubelet组件无法运行
ansible vms -m shell -a 'sed -i "/swap/s/^/#/" /etc/fstab && mount -a  && swapoff -a && free -m'
```

#### 开启IPV4路由转发

```
ansible vms -m shell -a 'sed -i "/net.ipv4.ip_forward = 0/s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/" /etc/sysctl.conf  && echo "net.bridge.bridge-nf-call-ip6tables = 1" >> /etc/sysctl.conf && echo "net.bridge.bridge-nf-call-iptables = 1" >> /etc/sysctl.conf && sysctl -p'
```

#### 启用cgroup

```
ansible vms -m shell -a 'echo GRUB_CMDLINE_LINUX_DEFAULT="cgroup_enable=memory swapaccount=1" >> /etc/default/grub && echo GRUB_CMDLINE_LINUX="cgroup_enable=memory swapaccount=1" >> /etc/default/grub'
cat >> /usr/bin/update-grub << END
#!/bin/bash
set -e
exec grub2-mkconfig -o /boot/grub2/grub.cfg "$@"
END
ansible vms -m synchronize -a 'src=/usr/bin/update-grub  dest=/usr/bin/'
ansible vms -m shell -a 'chmod +x /usr/bin/update-grub && update-grub'
```

### 工具准备

#### 安装RKE

```
#只需要给操作的主机安装即可,但是这里给所有主机都安装了
ansible vms -m shell -a 'wget https://github.com/rancher/rke/releases/download/v1.0.0/rke_linux-amd64 && chmod +x rke_linux-amd64 && mv rke_linux-amd64 /usr/bin/rke'
```

#### 安装kubectl

```
ansible vms -m shell -a 'curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl'
ansible vms -m shell -a 'chmod +x ./kubectl && mv ./kubectl /usr/bin/ && kubectl version --client'
```

#### 安装docker

```
#按照rancher官方快速安装脚本，但是安装18.09版本（推荐）
ansible vms -m shell -a 'curl https://releases.rancher.com/install-docker/18.09.sh | sh'
#Kubernetes1.8需要Docker 1.12.6、1.13.1、17.03，不支持更高版本的Docker
ansible vms -m shell -a 'yum install -y yum-utils  device-mapper-persistent-data lvm2'
ansible vms -m shell -a 'yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo'
ansible vms -m shell -a 'yum -y install docker-ce-17.03.0.ce-1.el7.centos docker-ce-cli-17.03.0.ce-1.el7.centos containerd.io'
ansible vms -m shell -a 'systemctl restart docker && systemctl enable docker && systemctl status docker'
```

#### 用户rancher及免密

```
#创建用户rancher
ansible vms -m shell -a 'grep docker /etc/group && useradd -G docker rancher'
ansible vms -m shell -a 'echo 123 | passwd --stdin rancher'
#root用户创建秘钥(131主机)
ssh-keygen -t rsa
#传输私钥（131主机）
for i in `seq 6 9`
do
ssh-copy-id -i  /root/.ssh/id_rsa.pub  rancher@192.168.220.$i
done
```

#### 安装helm

```
#只需要在操作主机安装即可（2.3主机）
#官方使用3版本（推荐）
ansible vms -m shell -a 'wget https://get.helm.sh/helm-v3.0.2-linux-amd64.tar.gz && tar -zvxf helm-v3.0.2-linux-amd64.tar.gz && mv linux-amd64/helm /usr/bin/'
```

## 创建kubernetets集群

### 创建rancher-cluster.yml文件

```
cd /home/rancher/
cat > rancher-cluster.yml << EOF
nodes: 
  - address: 192.168.220.6
    user: rancher
    role: [controlplane,worker,etcd]
  - address: 192.168.220.7
    user: rancher
    role: [worker,etcd]
  - address: 192.168.220.8
    user: rancher
    role: [worker,etcd]
    
services:  #包括etcd、kube-api、kube-controller、scheduler、kubelete、kubeproxy
  etcd:
    snapshot: true
    creation: 6h
    retention: 24h
EOF

#说明：
	#address:生产中为公网地址
	#user:廉洁到服务器的ssh用户
	#ssh_key_pass:~/.ssh/id_rsa执行命令的私钥路径，文件中会选择默认路径
	#internal_address:内网ip
	#controlplane角色：存放运行K8s所需的Kubernetes API服务器和其他组件
					 #启动HA集群，只需controlplan指定多个主机，然后正常启动集群即可
	#RKE是一个幂等工具，可以运行多次，且每次均产生相同的输出
	例如网路插件部署（以下三个均支持）
		Calico
		Flannel (default)
		Canal
#文件中的附加选项(下面内容配置非必须写进yml文件,会采用默认方案)docker
##使用不同的网络插件，可以在配置文件中指定：
network: 
  plugin: calico
##镜像信息指定：
system_images:
  flannel: rancher/coreos-flannel:v0.11.0  #默认网络模式就为flannel
  kubedns: rancher/k8s-dns-kube-dns-amd64:1.14.5  
  dnsmasq: rancher/k8s-dns-dnsmasq-nanny-amd64:1.14.5  
  kubedns_sidecar: rancher/k8s-dns-sidecar-amd64:1.14.5  
  kubedns_autoscaler: rancher/cluster-proportional-autoscaler-amd64:1.0.0  
  dashboard: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3
##认证模式：
authentication: 
  strategy: x509	#默认模式为x509
```

### RKE启动集群

```sh
rke up --config rancher-cluster.yml

#如果iptables报相关错误，尝试下面来解决
iptables -t nat -N DOCKER
iptables --wait -t nat -A DOCKER -p tcp -d 0/0 --dport 2380 -j DNAT --to-destination 172.17.0.2:1337 ! -i docker0
iptables -t filter -N DOCKER
iptables --wait -t filter -A DOCKER ! -i docker0 -o docker0 -p tcp -d 172.17.0.2 --dport 1337 -j ACCEPT	
#其他报错
docker rm $(docker stop $(docker ps -qa))
rm -rf /etc/ceph \
       /var/lib/docker 等
注意代理等设置
```

### 设置环境变量

```
echo 'export KUBECONFIG=/home/rancher/kube_config_rancher-cluster.yml' >> /etc/profile && source /etc/profile && tail -1 /etc/profile
```

### 查看pods

```
#查看pods，有kybe-system、ingress-nginx两个命名空间的pods
kubectl get pods --all-namespaces
#此时有五个pod没有启动起来，下面四个不用管，最上面的需要重新启动
[root@vm1 ~]# kubectl get pods --all-namespaces -o wide
NAMESPACE       NAME                                      READY   STATUS              RESTARTS   AGE    IP                NODE              NOMINATED NODE   READINESS GATES
ingress-nginx   default-http-backend-67cf578fc4-nhhqj     0/1     ContainerCreating   0          108m   <none>            192.168.168.132   <none>           <none>
...
#查看原因
[root@vm1 ~]# kubectl describe pod default-http-backend-67cf578fc4-nhhqj -n ingress-nginx
...
Events:
  Type     Reason      Age                    From                      Message
  ----     ------      ----                   ----                      -------
  Warning  FailedSync  4m35s (x309 over 89m)  kubelet, 192.168.168.132  error determining status: rpc error: code = DeadlineExceeded desc = context deadline exceeded
#显示code = DeadlineExceeded
#镜像拉取时间超出了kubelet的配置参数--image-pull-progress-deadlineshe默认的一分钟导致的
#重新执行
kubectl get pod $pod_name -n ingress-nginx -o yaml | kubectl replace --force -f -
```

### 在kubernetes集群上安装rancher

#### server的SS/TLS配置

```
#rancher依靠证书管理器从rancher自己生成的CA办法证书或者请求Let's Encrypt证书
#cert-manager仅由Rancher生成的CA（ingress.tls.source=rancher）和“让我们加密已发布的证书（ingress.tls.source=letsEncrypt）”颁发的证书才需要。如果您使用自己的证书文件（选项ingress.tls.source=secret），或者在外部负载均衡器上使用TLS终止，则应跳过此步骤
kubectl apply -f https://raw.githubusercontent.com/jetstack/cert-manager/release-0.12/deploy/manifests/00-crds.yaml
kubectl create namespace cert-manager
helm repo add jetstack https://charts.jetstack.io
	如果报错提示执行helm init 或者 helm init --client-only
	再执行 helm repo add jetstack https://charts.jetstack.io 
helm install   cert-manager jetstack/cert-manager   --namespace cert-manager   --version v0.12.0
kubectl get pods --namespace cert-manager
```

#### 添加helm图表库(包含下载rancher的图表)

```sh
#测试可以使用最新版本(推荐)
helm repo add rancher-latest https://releases.rancher.com/server-charts/latest
#生产推荐使用稳定版
helm repo add rancher-stable https://releases.rancher.com/server-charts/stable
```

#### 为rancher创建一个namespace

```sh
#为rancher创建命名空间
kubectl create namespace cattle-system
```

#### rancher创建rancher

```
helm install rancher rancher-latest/rancher   --namespace cattle-system   --set hostname=vm-rancher.com
#报错Error: failed to download "rancher-stable/rancher" (hint: running `helm repo update` may help)
#按照报错提示运行
helm repo update  #运行无报错，但是创建rancher继续报错

#尝试helm的稳定版rancher图表，再部署rancher成功
helm repo add rancher-latest https://releases.rancher.com/server-charts/stable

访问
https://vm-rancher.com/
```

#### cattle-xxx-agent CrashLoopBackOff

```sh
1.部署完成后，其中的cattle-cluster-agent和cattle-node-agent一直起不来
[root@vm6 rancher]# kubectl get pods --all-namespaces
NAMESPACE       NAME                                      READY   STATUS             RESTARTS   AGE
cattle-system   cattle-cluster-agent-78ddc5949d-x9xp5     0/1     CrashLoopBackOff   196        16h
cattle-system   cattle-node-agent-88887                   0/1     CrashLoopBackOff   196        16h
cattle-system   cattle-node-agent-bclwt                   0/1     Error              196        16h
cattle-system   cattle-node-agent-vjrbf                   0/1     CrashLoopBackOff   196        16h
2.查看日志报错
ERROR: https://vm-rancher.com/ping is not accessible (Could not resolve host: vm-rancher.com)
3.分析原因
如果没有内部DNS服务器，而是通过添加/etc/hosts主机别名的方式指定的Rancher Server域名，那么无论通过哪种方式（自定
义、导入、Host驱动等）创建k8s集群，k8s集群运行起来之后，因为cattle-cluster-agent和cattle-node-agent无法通过
DNS记录找到Rancher Server，而最终导致无法通信
4.解决（为pod天增加主机名）
给两个pod分别打补丁patch
语法：
kubectl patch deployment patch-demo --patch "$(cat patch-file-containers.yaml)"

export KUBECONFIG=xxx/xxx/xx.kubeconfig.yaml #指定kubectl配置文件
#配置 cattle-cluster-agent
kubectl -n cattle-system patch  deployments cattle-cluster-agent --patch '{
    "spec": {
        "template": {
            "spec": {
                "hostAliases": [
                    {
                        "hostnames":
                        [
                            "vm-rancher.com"
                        ],
                            "ip": "192.168.174.3"
                    }
                ]
            }
        }
    }
}'
#配置 cattle-node-agent
export KUBECONFIG=xxx/xxx/xx.kubeconfig.yaml #指定kubectl配置文件
kubectl -n cattle-system patch  daemonsets cattle-node-agent --patch '{
    "spec": {
        "template": {
            "spec": {
                "hostAliases": [
                    {
                        "hostnames":
                        [
                            "vm-rancher.com"
                        ],
                            "ip": "192.168.174.3"
                    }
                ]
            }
        }
    }
}'
5.配置/etc/hosts
192.168.220.6 vm-rancher.com
```

#### 监控问题

```sh
1.#启用监控时不显示有时候监控组件版本，此时就算点击启用，也会失败，主要原因是因为获取不到rancher的system-charts，在
#全局下点击【工具】、【商店】，显示的system-library的url为https://git.rancher.io/system-charts，一般为不能访问此url
#导致的。局域网环境获取不到，找官方文档修改地址，偶然性访问不到，可以在【启用监控】页面多刷新几次，最直接的解决办法
#就是clone官方的仓库，然后配置为自己的地址
官方仓库地址：https://github.com/rancher/system-charts.git
注意分支，如果为release-v2.3则需要使用release-v2.3分支，改为自己地址后可以再试试
2.#监控一直处于未就绪状态
查看rancher日志时候发现找不到endpoint cattle-prometheus/prometheus-operated
2.1此时查看 namespace=cattle-prometheus的所有资源，这里是正常的
[root@vm6 rancher]# kubectl get all -n=cattle-prometheus
NAME                                                           READY   STATUS    RESTARTS   AGE
pod/exporter-kube-state-cluster-monitoring-6db64f7d4b-qtljz    1/1     Running   0          20m
pod/exporter-node-cluster-monitoring-54bj6                     1/1     Running   0          20m
pod/exporter-node-cluster-monitoring-78nb2                     1/1     Running   0          20m
pod/exporter-node-cluster-monitoring-l88dq                     1/1     Running   0          20m
pod/grafana-cluster-monitoring-65cbc8749-7x8tv                 2/2     Running   0          20m
pod/prometheus-cluster-monitoring-0                            5/5     Running   1          19m
pod/prometheus-operator-monitoring-operator-5b66559965-c74z6   1/1     Running   0          20m

NAME                                TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE
service/access-grafana              ClusterIP   10.43.199.86   <none>        80/TCP              20m
service/access-prometheus           ClusterIP   10.43.139.8    <none>        80/TCP              20m
service/expose-grafana-metrics      ClusterIP   None           <none>        3000/TCP            20m
service/expose-kubelets-metrics     ClusterIP   None           <none>        10250/TCP           20m
service/expose-kubernetes-metrics   ClusterIP   None           <none>        8080/TCP,8081/TCP   20m
service/expose-node-metrics         ClusterIP   None           <none>        9796/TCP            20m
service/expose-operator-metrics     ClusterIP   None           <none>        47323/TCP           20m
service/expose-prometheus-metrics   ClusterIP   None           <none>        9090/TCP            20m
service/prometheus-operated         ClusterIP   None           <none>        9090/TCP            19m

NAME                                              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/exporter-node-cluster-monitoring   3         3         3       3            3           kubernetes.io/os=linux   20m

NAME                                                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/exporter-kube-state-cluster-monitoring    1/1     1            1           20m
deployment.apps/grafana-cluster-monitoring                1/1     1            1           20m
deployment.apps/prometheus-operator-monitoring-operator   1/1     1            1           20m

NAME                                                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/exporter-kube-state-cluster-monitoring-6db64f7d4b    1         1         1       20m
replicaset.apps/grafana-cluster-monitoring-65cbc8749                 1         1         1       20m
replicaset.apps/prometheus-operator-monitoring-operator-5b66559965   1         1         1       20m

NAME                                             READY   AGE
statefulset.apps/prometheus-cluster-monitoring   1/1     19m
2.2查看所有的endpoint
#这里演示正常，非正常情况下可以发现其中expose-kubelets-metrics会有节点没起来10255端口，一般情况是因为该节点上缺
#少镜像有 rancher/coreos-prometheus-operator ，下载该镜像，然后禁用监控，再启用监控，此时资源信息一切正常
[root@vm6 rancher]# kubectl get endpoints --all-namespaces
NAMESPACE           NAME                        ENDPOINTS                                                                 AGE
cattle-prometheus   access-grafana              10.42.2.5:8080                                                            25m
cattle-prometheus   access-prometheus           10.42.0.10:8080                                                           25m
cattle-prometheus   expose-grafana-metrics      10.42.2.5:3000                                                            25m
cattle-prometheus   expose-kubelets-metrics     192.168.220.6:10255,192.168.220.7:10255,192.168.220.8:10255 + 6 more...   24m
cattle-prometheus   expose-kubernetes-metrics   10.42.1.8:8081,10.42.1.8:8080                                             25m
cattle-prometheus   expose-node-metrics         192.168.220.6:9796,192.168.220.7:9796,192.168.220.8:9796                  25m
cattle-prometheus   expose-operator-metrics     10.42.1.6:8080                                                            25m
cattle-prometheus   expose-prometheus-metrics   10.42.0.10:9090                                                           25m
cattle-prometheus   prometheus-operated         10.42.0.10:9090                                                           24m
cattle-system       rancher                     10.42.0.5:80,10.42.1.5:80,10.42.2.4:80                                    19h
cert-manager        cert-manager                10.42.0.4:9402                                                            19h
cert-manager        cert-manager-webhook        10.42.2.3:10250                                                           19h
default             kubernetes                  192.168.220.6:6443                                                        19h
ingress-nginx       default-http-backend        10.42.1.3:8080                                                            19h
kube-system         kube-controller-manager     <none>                                                                    19h
kube-system         kube-dns                    10.42.0.2:53,10.42.1.4:53,10.42.0.2:53 + 3 more...                        19h
kube-system         kube-scheduler              <none>                                                                    19h
kube-system         metrics-server              10.42.2.2:443                                                             19h
```

### 添加或删除节点

```sh
1.#RKE支持为角色为worker和controlplane的主机添加或删除节点
添加节点：
	修改原始cluster.yml文件，指定节点运行的角色
删除节点：
	修改原始cluster.yml文件
修改配置后：
	运行 rke up --config rancher-cluster.yml
2.#仅添加/删除工作worker节点
在配置文件中添加/删除工作节点后运行 rke up --update-only --config rancher-cluster.yml ，除了工作节点修改之
其他的修改将被忽略 
```

### 删除集群

```sh
#RKE支持rke remove命令，这个命令是不可逆的，它将彻底摧毁Kubernetes集群
这个命令对rancher-cluster.yml中的每个节点执行以下操作：
	删除部署在其上的Kubernetes组件
		etcd
		kube-apiserver
		kube-controller-manager
		kubelet
		kube-proxy
		nginx-proxy
	清除每台主机中服务留下的目录
		/etc/kubernetes/ssl
		/var/lib/etcd
		/etc/cni
		/opt/cni
		/var/run/calico
```

目前这份文档的ssl出现问题，未解决

```sh
#查看相关证书详情
 openssl s_client -showcerts -connect 192.168.2.3:443 </dev/null
 #访问报错的404 - default backend （下面是可能原因）
 1.Nginx ingress无法为rancher-cluster.yml配置的应用提供代理服务
 这应该是您为访问Rancher的FQDN配置错误，您可以通过运行以下命令来检查它是否已正确配置
 kubectl --kubeconfig kube_config_rancher-cluster.yml get ingress -n cattle-system -o wide
 检查HOSTS是否与配置文件的<FQDN>相同，以及列出的ADDRESS是否与配置文件配置的主机IP相同
 2.通过以下命令查看nginx ingress日志:
 kubectl --kubeconfig kube_config_rancher-cluster.yml logs -l app=ingress-nginx -n ingress-nginx
 3.x509: certificate is valid for fqdn, not your_configured_fqdn
 证书中包含的域名与实际的域名不匹配
 4.Port 80 is already in use. Please check the flag --http-port
 5.unexpected error creating pem file: no valid PEM formatted block found
```

